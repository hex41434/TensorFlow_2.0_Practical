TensorFlow 2.0 enabled ***Distributed Strategy***. You can develop your model once and then decide whether to run it over multiple GPUs or even CPUs, dramatically improving computational efficiency with only a small amount of extra code.

Follow this link to the TensorFlow documentation about distributed training:
https://www.tensorflow.org/guide/distributed_training

In ***Google Colab***, you also have the option to change runtime, so you don’t need to own GPUs or TPUs to take advantage of using them in the cloud.

A ***TPU is a tensor processing unit***. Wikipedia defines a TPU as “an AI accelerator application-specific integrated circuit (ASIC) developed by Google specifically for neural network machine learning.” It is very efficient and about ten times faster than regular CPUs.
https://en.wikipedia.org/wiki/Tensor_processing_unit
