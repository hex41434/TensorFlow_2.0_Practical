# -*- coding: utf-8 -*-
"""Bonus 3: Detect Smiling Faces Using Deep Learning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BE9Wugvud78_TMFpBkFzmI2jI3HGdLbw

# PROBLEM STATEMENT

<table>
  <tr><td>
    <img src="https://upload.wikimedia.org/wikipedia/commons/8/88/Yellow_Happy.jpg"
         alt="Happy Faces"  width="400">
  </td></tr>
  <tr><td align="center">
    <b>Figure 1. Happy Faces Classification
  </td></tr>
</table>

- The dataset contains a series of images that can be used to solve the Happy House problem! 
- We need to build an artificial neural network that can detect smiling faces.
- Only smiling people will be allowed to enter the house!
- The train set has 600 examples. The test set has 150 examples.
- Data Source: https://www.kaggle.com/iarunava/happy-house-dataset
"""

from google.colab import drive
drive.mount('/content/drive')

"""# STEP #0: IMPORT LIBRARIES"""

!pip install tensorflow-gpu==2.0.0-beta1

# import libraries 
import pandas as pd # Import Pandas for data manipulation using dataframes
import numpy as np # Import Numpy for data statistical analysis 
import matplotlib.pyplot as plt # Import matplotlib for data visualisation
import seaborn as sns
import h5py
import random

"""# STEP #1: IMPORT DATASETS AND NORMALIZE IT"""

filename = '/content/drive/My Drive/Colab Notebooks/train_happy.h5'
f = h5py.File(filename, 'r')

for key in f.keys():
    print(key) #Names of the groups in HDF5 file.

happy_training = h5py.File('/content/drive/My Drive/Colab Notebooks/train_happy.h5', "r")
happy_testing  = h5py.File('/content/drive/My Drive/Colab Notebooks/test_happy.h5', "r")

X_train = np.array(happy_training["train_set_x"][:]) 
y_train = np.array(happy_training["train_set_y"][:]) 

X_test = np.array(happy_testing["test_set_x"][:])
y_test = np.array(happy_testing["test_set_y"][:])

X_train

X_train.shape

y_train

y_train.shape

"""# STEP #2: VISUALIZE DATASET"""

i = random.randint(1,600) # select any random index from 1 to 600
plt.imshow( X_train[i] )
print(y_train[i])

# Let's view more images in a grid format
# Define the dimensions of the plot grid 
W_grid = 5
L_grid = 5

# fig, axes = plt.subplots(L_grid, W_grid)
# subplot return the figure object and axes object
# we can use the axes object to plot specific figures at various locations

fig, axes = plt.subplots(L_grid, W_grid, figsize = (25,25))

axes = axes.ravel() # flaten the 15 x 15 matrix into 225 array

n_training = len(X_train) # get the length of the training dataset

# Select a random number from 0 to n_training
for i in np.arange(0, W_grid * L_grid): # create evenly spaces variables 

    # Select a random number
    index = np.random.randint(0, n_training)
    # read and display an image with the selected index    
    axes[i].imshow( X_train[index])
    axes[i].set_title(y_train[index], fontsize = 25)
    axes[i].axis('off')

plt.subplots_adjust(hspace=0.4)

"""# STEP #3: DATA PREPARATION"""

# Let's normalize dataset
X_train = X_train/255
X_test = X_test/255

X_train

plt.imshow(X_train[9])

X_train.shape

y_train.shape

"""# STEP#4: MODEL TRAINING"""

import tensorflow as tf

cnn = tf.keras.Sequential()

cnn.add(tf.keras.layers.Conv2D(64, (6,6), activation = 'relu', input_shape = (64,64,3)))
cnn.add(tf.keras.layers.MaxPooling2D(2,2))

# cnn.add(tf.keras.layers.Dropout(0.2))


cnn.add(tf.keras.layers.Conv2D(64, (5,5), activation = 'relu'))
cnn.add(tf.keras.layers.MaxPooling2D(2,2))

cnn.add(tf.keras.layers.Flatten())

cnn.add(tf.keras.layers.Dense(128, activation = 'relu'))

cnn.add(tf.keras.layers.Dense(64, activation = 'relu'))

cnn.add(tf.keras.layers.Dense(1, activation = 'sigmoid'))
cnn.summary()

cnn.compile(optimizer = 'Adam', loss = 'binary_crossentropy', metrics = ['accuracy'])

history = cnn.fit(X_train,
                 y_train, 
                 batch_size = 30,
                 nb_epoch = 50,
                 verbose = 1)

"""# STEP#5: MODEL EVALUATION"""

evaluation = cnn.evaluate(X_test, y_test)
print('Test Accuracy : {:.3f}'.format(evaluation[1]))

history.history.keys()

accuracy = history.history['accuracy']
loss = history.history['loss']

epochs = range(len(accuracy))

plt.plot(epochs, accuracy, 'bo', label='Training Accuracy')
plt.title('Training Accuracy')
plt.legend()

plt.plot(epochs, loss, 'ro', label='Training loss')
plt.title('Training loss')
plt.legend()

predicted_classes = cnn.predict_classes(X_test)
y_true = y_test

from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_true, predicted_classes)
plt.figure(figsize = (5, 5))
sns.heatmap(cm, annot = True)

L = 5
W = 5
fig, axes = plt.subplots(L, W, figsize = (12,12))
axes = axes.ravel() # 

for i in np.arange(0, L * W):  
    axes[i].imshow(X_test[i])
    axes[i].set_title("Prediction Class = {}\n True Class = {}".format(predicted_classes[i], y_test[i]))
    axes[i].axis('off')

plt.subplots_adjust(wspace=0.5)

# axes[i].set_title("Guess{}\n True{}".format(predicted_class[i], y_test[i]))

from sklearn.metrics import classification_report

print(classification_report(y_test.T, predicted_classes))

"""# GREAT JOB!"""