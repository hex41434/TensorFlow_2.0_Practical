One of the most common problems while training deep neural networks is ***overfitting***. “Overfitting occurs when a statistical model or machine learning algorithm captures the noise of the data. When an overfitted model sees data it hasn't seen before, it will fail miserably. Intuitively, overfitting occurs when the model or the algorithm fits the data too well. Specifically, overfitting occurs if the model or algorithm shows low bias but high variance. Overfitting is often a result of an excessively complicated model, and it can be prevented by fitting multiple models and using validation or cross-validation to compare their predictive accuracies on test data.” https://chemicalstatistician.wordpress.com/2014/03/19/machine-learning-lesson-of-the-day-overfitting-and-underfitting/ 

***Regularization*** comes into play with regards to creating a network that performs well on both training and test data. “In mathematics, statistics, and computer science, particularly in machine learning and inverse problems, regularization is the process of adding information in order to solve an ill-posed problem or to prevent overfitting.” https://en.wikipedia.org/wiki/Regularization_(mathematics)
