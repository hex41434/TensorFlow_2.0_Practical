# -*- coding: utf-8 -*-
"""Copy of Bonus 4: Deploy Smiling Face Model Using TF2.0 Serving.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1F3bU3f2aqO-qGCOpDlcBkDzDHOId9GV4

# STEP #0: PROBLEM STATEMENT

- In this project, we will build, train and test a model to classify smiling faces using TensorFlow 2.0 similar to what we have already done in the past
- What's new is that we are going to deploy the model in practice using Tensorflow Serving. 

- Note: TF serving code section is inspired by the TensorFlow 2.0 Documentation: https://www.tensorflow.org/tfx/serving/tutorials/Serving_REST_simple

## STEP #1: IMPORT PACKAGES
"""

from google.colab import drive
drive.mount('/content/drive')

import tensorflow as tf
from tensorflow import keras

import random
import json
import numpy as np
import matplotlib.pyplot as plt
import os

# import libraries 
import pandas as pd # Import Pandas for data manipulation using dataframes
import numpy as np # Import Numpy for data statistical analysis 
import matplotlib.pyplot as plt # Import matplotlib for data visualisation
import seaborn as sns
import h5py
import random

"""# STEP #2: IMPORT DATASET"""

filename = '/content/drive/My Drive/Colab Notebooks/train_happy.h5'
f = h5py.File(filename, 'r')

for key in f.keys():
    print(key) #Names of the groups in HDF5 file.

happy_training = h5py.File('/content/drive/My Drive/Colab Notebooks/train_happy.h5', "r")
happy_testing  = h5py.File('/content/drive/My Drive/Colab Notebooks/test_happy.h5', "r")

X_train = np.array(happy_training["train_set_x"][:]) 
y_train = np.array(happy_training["train_set_y"][:]) 

X_test = np.array(happy_testing["test_set_x"][:])
y_test = np.array(happy_testing["test_set_y"][:])

train_images = X_train 
train_labels = y_train
test_images = X_test 
test_labels = y_test

train_images.shape

i = random.randint(1,600) # select any random index from 1 to 600
plt.imshow( train_images[i] )
print(train_labels[i])

# Normalize data
train_images = train_images / 255.0
test_images = test_images / 255.0

train_images.shape

test_images.shape

class_names = ['Sad', 'Happy']

train_labels

"""# STEP #3: BUILD, TRAIN AND TEST THE MODEL"""

from tensorflow.keras import datasets, layers, models

model = models.Sequential()

model.add(layers.Conv2D(64, (6,6), activation = 'relu', input_shape = (64,64,3)))
model.add(layers.MaxPooling2D(2,2))

model.add(layers.Dropout(0.2))


model.add(layers.Conv2D(64, (5,5), activation = 'relu'))
model.add(layers.MaxPooling2D(2,2))

model.add(layers.Flatten())

model.add(layers.Dense(128, activation = 'relu'))

model.add(layers.Dense(64, activation = 'relu'))

model.add(layers.Dense(1, activation = 'sigmoid'))
model.summary()

epochs = 30

model.compile(optimizer=tf.train.AdamOptimizer(), 
              loss='binary_crossentropy',
              metrics=['accuracy'])

train_images.shape

train_labels.shape

model.fit(train_images, train_labels, epochs=epochs)

test_loss, test_acc = model.evaluate(test_images, test_labels)
print('\nTest accuracy: {}'.format(test_acc))

"""# STEP #4: SAVE THE MODEL

- We now need to save our trained model and it has to be saved in a SavedModel format.
- The model will have a version number and will be saved in a structured directory 
- tf.saved_model.simple_save is a function used to build a saved model that is suitable for serving using Tensorflow Serving. 
- After the model is saved, we can now use TensorFlow Serving to start making inference requests using a specific version of our trained model "servable".
- Use SavedModel to save and load your modelâ€”variables, the graph, and the graph's metadata. 
- This is a language-neutral, recoverable, hermetic serialization format that enables higher-level systems and tools to produce, consume, and transform TensorFlow models.
- simple_save offers a very easy way to save a model as follows: 

  - simple_save(session,
            export_dir,
            inputs={"x": x, "y": y},
            outputs={"z": z})
            
- Check this out for more information: 
https://www.tensorflow.org/guide/saved_model
- Note: TF serving code section is inspired by the TensorFlow 2.0 Documentation: https://www.tensorflow.org/tfx/serving/tutorials/Serving_REST_simple
"""

# Let's obtain a temporary storage directory
import tempfile
MODEL_DIR = tempfile.gettempdir()

MODEL_DIR

# Let's specify the model version, choose #1 for now 
version = 1

# Let's join the temp model directory with our chosen version number 
# The expected result will be = '\tmp\version number'
export_path = os.path.join(MODEL_DIR, str(version))
print('export_path = {}\n'.format(export_path))

# Let's save the model using simple_save
# If the directory already exists, we will remove it using '!rm' 
# rm removes each file specified on the command line. 

if os.path.isdir(export_path):
  print('\nAlready saved a model, cleaning up\n')
  !rm -r {export_path}

tf.saved_model.simple_save(
    keras.backend.get_session(),
    export_path,
    inputs={'input_image': model.input},
    outputs={t.name:t for t in model.outputs})

print('\nSaved model:')

!ls -l {export_path}

"""# STEP #5: EXPLORE OUR SAVED MODEL

- saved_model_cli will be used to explore MetaGraphDefs (the models) and SignatureDefs (the methods you can call) in our SavedModel. 
- A MetaGraph is a dataflow graph, plus its associated variables, assets, and signatures. 
- A signature is the set of inputs to and outputs from a graph.
"""

# Now we can view our saved model
!saved_model_cli show --dir {export_path} --all

"""# STEP #6: SERVE THE MODEL USING TESNSORFLOW SERVING

# STEP 6.1: Let's add tensorflow-model-server package to our list of packages
"""

!echo "deb http://storage.googleapis.com/tensorflow-serving-apt stable tensorflow-model-server tensorflow-model-server-universal" | tee /etc/apt/sources.list.d/tensorflow-serving.list && \
curl https://storage.googleapis.com/tensorflow-serving-apt/tensorflow-serving.release.pub.gpg | apt-key add -
!apt update

"""# Step 6.2: Let's install tensorflow model server:"""

!apt-get install tensorflow-model-server

"""# Step 6.3: Let's run TensorFlow serving

- We will load our model and start making inferences (predictions) based on it.
- There are some important parameters:

  - rest_api_port: The port that you'll use for REST requests.
  - model_name: You'll use this in the URL of REST requests. You can choose any name
  - model_base_path: This is the path to the directory where you've saved your model.
  
- For more information regarding REST, check this out: 
https://www.codecademy.com/articles/what-is-rest
- REST is a revival of HTTP in which http commands have semantic meaning.
"""

os.environ["MODEL_DIR"] = MODEL_DIR

# Commented out IPython magic to ensure Python compatibility.
# %%bash --bg 
# nohup tensorflow_model_server \
#   --rest_api_port=8501 \
#   --model_name=smiling_model \
#   --model_base_path="${MODEL_DIR}" >server.log 2>&1

!tail server.log

"""- **Congratulations! now we have successfully loaded a servable version of our model {name: fashion_model version: 1}**

# STEP 6.4: Let's start making requests in TensorFlow Serving
"""

def show(idx, title):
  plt.figure()
  plt.imshow(test_images[idx].reshape(64,64, 3))
  plt.title('\n\n{}'.format(title), fontdict={'size': 16})

rando = random.randint(0,len(test_images)-1)
show(rando, 'An Example Image: {}'.format(class_names[test_labels[rando]]))

# Let's create a JSON object and make 3 inference requests
data = json.dumps({"signature_name": "serving_default", "instances": test_images[0:10].tolist()})
print('Data: {} ... {}'.format(data[:50], data[len(data)-52:]))

!pip install -q requests

import requests

headers = {"content-type": "application/json"}
json_response = requests.post('http://localhost:8501/v1/models/smiling_model:predict', data=data, headers=headers)
predictions = json.loads(json_response.text)['predictions']

show(0, 'The model thought this was a {} (class {}), and it was actually a {} (class {})'.format(
  class_names[int(np.round(predictions[0]))], int(np.round(predictions[0])), class_names[test_labels[0]], test_labels[0]))

for i in range(0,9):
  show(i, 'The model thought this was a {} (class {}), and it was actually a {} (class {})'.format(
    class_names[int(np.round(predictions[i]))], int(np.round(predictions[i])), class_names[test_labels[i]], test_labels[i]))